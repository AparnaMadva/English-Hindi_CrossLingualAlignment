{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgyNKeqBKabI",
        "outputId": "a7a6f831-7f88-46f3-b3ec-6bbf47cd1362"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# restart session before running this cell\n",
        "import gensim\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "_O8vtFOlJqdQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Preperation"
      ],
      "metadata": {
        "id": "F8AshKnKJevR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tovMcrt1E9DO",
        "outputId": "8ede13f3-0259-4f26-a4d3-ddc0569cb723"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-07 10:07:41--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.34.53, 13.226.34.7, 13.226.34.122, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.34.53|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1325960915 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.en.300.vec.gz.1’\n",
            "\n",
            "cc.en.300.vec.gz.1  100%[===================>]   1.23G   157MB/s    in 10s     \n",
            "\n",
            "2025-04-07 10:07:51 (126 MB/s) - ‘cc.en.300.vec.gz.1’ saved [1325960915/1325960915]\n",
            "\n",
            "--2025-04-07 10:07:51--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.34.53, 13.226.34.7, 13.226.34.122, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.34.53|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1118942272 (1.0G) [binary/octet-stream]\n",
            "Saving to: ‘cc.hi.300.vec.gz.1’\n",
            "\n",
            "cc.hi.300.vec.gz.1  100%[===================>]   1.04G   124MB/s    in 9.8s    \n",
            "\n",
            "2025-04-07 10:08:01 (109 MB/s) - ‘cc.hi.300.vec.gz.1’ saved [1118942272/1118942272]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using pre-trained FastText embeddings for English and Hindi languages\n",
        "english_embeddings = KeyedVectors.load_word2vec_format('cc.en.300.vec.gz', limit=100000)\n",
        "hindi_embeddings = KeyedVectors.load_word2vec_format('cc.hi.300.vec.gz', limit=100000)"
      ],
      "metadata": {
        "id": "zfWo_B2LX0yF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"number of English and Hindi embeddings: \")\n",
        "print(len(english_embeddings.index_to_key))\n",
        "print(len(hindi_embeddings.index_to_key))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOQ3PQRTdBhi",
        "outputId": "f698aee9-924b-457e-9798-2b9acedf3e83"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of English and Hindi embeddings: \n",
            "100000\n",
            "100000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the English-Hindi bilingual lexicon from MUSE train dataset\n",
        "!wget https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.txt\n",
        "\n",
        "# MUSE test dataset\n",
        "!wget https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.5000-6500.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q-0755Pc7Ar",
        "outputId": "225c3dbe-d2c5-4231-cf5a-e4788a5dc1a7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-07 10:10:22--  https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.txt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.34.122, 13.226.34.53, 13.226.34.7, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.34.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 930856 (909K) [text/x-c++]\n",
            "Saving to: ‘en-hi.txt.1’\n",
            "\n",
            "\ren-hi.txt.1           0%[                    ]       0  --.-KB/s               \ren-hi.txt.1         100%[===================>] 909.04K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-04-07 10:10:22 (21.3 MB/s) - ‘en-hi.txt.1’ saved [930856/930856]\n",
            "\n",
            "--2025-04-07 10:10:22--  https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.5000-6500.txt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.34.122, 13.226.34.53, 13.226.34.7, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.34.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 52464 (51K) [text/plain]\n",
            "Saving to: ‘en-hi.5000-6500.txt.1’\n",
            "\n",
            "en-hi.5000-6500.txt 100%[===================>]  51.23K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2025-04-07 10:10:22 (6.04 MB/s) - ‘en-hi.5000-6500.txt.1’ saved [52464/52464]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the English-Hindi bilingual lexicon from the downloaded .txt file\n",
        "# the final lexicon contains word pairs like (english_word, hindi_word)\n",
        "def read_muse(file_path):\n",
        "  enhi_lexicon = []\n",
        "  with open(file_path,'r') as f:\n",
        "    for entry in f:\n",
        "      english_word, hindi_word = entry.split()\n",
        "      if english_word in english_embeddings.index_to_key and hindi_word in hindi_embeddings.index_to_key:\n",
        "        enhi_lexicon.append((english_word,hindi_word))\n",
        "    return enhi_lexicon\n",
        "\n",
        "enhi_lexicon = read_muse('en-hi.txt')"
      ],
      "metadata": {
        "id": "u0nYz5DvMmi1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embedding Alignment"
      ],
      "metadata": {
        "id": "WTqDr2hrJZIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the english and hindi matrices for alignment using the MUSE en-hi lexicon and the pre-trained FastText embeddings\n",
        "def prepare_x_and_y(enhi_lexicon, english_embeddings, hindi_embeddings):\n",
        "  X, Y = [], []\n",
        "  for entry in enhi_lexicon:\n",
        "    en_word, hi_word = entry[0], entry[1]\n",
        "    if en_word in english_embeddings.index_to_key and hi_word in hindi_embeddings.index_to_key:\n",
        "      X.append(english_embeddings[en_word])\n",
        "      Y.append(hindi_embeddings[hi_word])\n",
        "  X, Y = np.array(X, dtype=np.float32), np.array(Y, dtype=np.float32)\n",
        "\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "GXO-K_ooZzHw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_procrustes(x,y):\n",
        "  u, s, vt = np.linalg.svd(np.dot(x.T,y))\n",
        "  w = np.dot(u,vt)\n",
        "  return w"
      ],
      "metadata": {
        "id": "xRxhBweWRjbz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the matrices X and Y to learn the transformation matrix W\n",
        "X, Y = prepare_x_and_y(enhi_lexicon, english_embeddings, hindi_embeddings)\n",
        "\n",
        "W = compute_procrustes(X, Y)"
      ],
      "metadata": {
        "id": "hD8KwphaZlZ4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of aligned pairs: \"+str(len(X)))\n",
        "print(\"Shape of the transformation matrix: \"+str(W.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rhbttb1Lw5Oa",
        "outputId": "8279dc0e-8cfa-47a9-d676-99d258dfadb2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of aligned pairs: 18972\n",
            "shape of the transformation matrix: (300, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aligned_english_embeddings = np.dot(english_embeddings.vectors,W).astype(np.float32)"
      ],
      "metadata": {
        "id": "RR8ek45Xb6kn"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify the translation using an example\n",
        "word = 'cat'\n",
        "en_vec = aligned_english_embeddings[english_embeddings.key_to_index[word]]\n",
        "similar_results = cosine_similarity(en_vec.reshape(1,-1), hindi_embeddings.vectors)\n",
        "most_similar = hindi_embeddings.index_to_key[similar_results.argmax()]\n",
        "\n",
        "print('English word: '+word+'\\n'+'Hindi word (translated): '+most_similar)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDOo3R268-41",
        "outputId": "ad4a9e51-678e-4581-ca1f-7bec32fff6d8"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English word: cat\n",
            "Hindi word (translated): बिल्ली\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "muse_test_set = read_muse('en-hi.5000-6500.txt')\n",
        "# muse_test_set = enhi_lexicon[1500:3000]\n",
        "\n",
        "X_test, Y_test = prepare_x_and_y(muse_test_set, english_embeddings, hindi_embeddings)\n",
        "print(\"Number of pairs in the test set: \"+str(len(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81V8pAY0J1CQ",
        "outputId": "a520ca1e-bb40-4bce-8aea-7f80770e416a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of pairs in the test set: 1600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aligned_test_embeddings = np.dot(X_test,W).astype(np.float32)"
      ],
      "metadata": {
        "id": "_djeIiKYLvxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation"
      ],
      "metadata": {
        "id": "us9JObE0JNye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation: calculate Precision@1 and Precision@5 metrics for word translation\n",
        "def calculate_precision(test_data, aligned_embeddings, hindi_embeddings):\n",
        "  first = 0\n",
        "  fifth = 0\n",
        "  p = 5\n",
        "  for word_en, word_hi in test_data:\n",
        "    en_vec = aligned_english_embeddings[english_embeddings.key_to_index[word]]\n",
        "    similar_results = cosine_similarity(en_vec.reshape(1,-1), hindi_embeddings.vectors)\n",
        "    # indices = similar_results[0].argsort()[-p:][::-1]\n",
        "    indices = similar_results[0].argsort()[-p:][::-1]\n",
        "    words = [hindi_embeddings.index_to_key[i] for i in indices]\n",
        "    if word_hi==words[0]:\n",
        "      first += 1\n",
        "    if word_hi in words:\n",
        "      fifth += 1\n",
        "  first_precision = first / len(test_data)\n",
        "  fifth_precision = fifth / len(test_data)\n",
        "\n",
        "  return first_precision, fifth_precision"
      ],
      "metadata": {
        "id": "KN1A_bd6_MBA"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision_at_1, precision_at_5 = calculate_precision(muse_test_set, aligned_english_embeddings, hindi_embeddings)\n",
        "\n",
        "print('Precision@1= '+str(precision_at_1))\n",
        "print('Precision@5= '+str(precision_at_5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3IW0f12i-Mp",
        "outputId": "ca7bb585-4e83-4731-e539-2bd0b9384998"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision@1= 0.0\n",
            "Precision@5= 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation: cosine similarities between word pairs to assess cross-lingual semantic similarity\n",
        "def calculate_similarities(bi_lexicon, aligned_embeddings, hindi_embeddings, pairs=10):\n",
        "  similarity = []\n",
        "  i = 0\n",
        "\n",
        "  for word_en, word_hi in bi_lexicon:\n",
        "    if word_en in english_embeddings and word_hi in hindi_embeddings:\n",
        "      en_vec = aligned_embeddings[english_embeddings.key_to_index[word_en]]\n",
        "      hi_vec = hindi_embeddings[word_hi]\n",
        "\n",
        "      sim = cosine_similarity(en_vec.reshape(1,-1), hi_vec.reshape(1,-1))\n",
        "      similarity.append((word_en, word_hi, sim))\n",
        "\n",
        "    i+=1\n",
        "    if i>=pairs:\n",
        "      break\n",
        "  return similarity"
      ],
      "metadata": {
        "id": "GFvSPCotJ_-F"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_pair_cosine_similarities = calculate_similarities(enhi_lexicon, aligned_english_embeddings, hindi_embeddings)\n",
        "\n",
        "print('{English word}  {Hindi word}  {Similarity}')\n",
        "for en,hi,sim in word_pair_cosine_similarities:\n",
        "  print(f\"{en}  {hi}  {sim}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9u6BDsrf-ME",
        "outputId": "86e6d5f3-a21e-4274-cd55-5c9e70e2eecc"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{English word}  {Hindi word}  {Similarity}\n",
            "and  और  [[0.14246619]]\n",
            "was  था  [[0.42320135]]\n",
            "was  थी  [[0.410759]]\n",
            "for  लिये  [[0.26180744]]\n",
            "that  उस  [[0.35282412]]\n",
            "that  कि  [[0.1700798]]\n",
            "with  साथ  [[0.18845299]]\n",
            "from  से  [[0.12805238]]\n",
            "from  इससे  [[0.26954097]]\n",
            "this  ये  [[0.22080687]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation: ablation study to assess the impact of bilingual lexicon size on alignment quality\n",
        "def ablation_study(bi_lexicon, english_embeddings, hindi_embeddings, dictionary_size=[5000,10000,20000]):\n",
        "  ablation_results = []\n",
        "\n",
        "  for size in dictionary_size:\n",
        "    lexicon = bi_lexicon[:size]\n",
        "    X, Y = [], []\n",
        "    for entry in lexicon:\n",
        "      en_word, hi_word = entry[0], entry[1]\n",
        "      X.append(english_embeddings[en_word])\n",
        "      Y.append(hindi_embeddings[hi_word])\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "\n",
        "    W = compute_procrustes(X, Y)\n",
        "    aligned_english_embeddings = np.dot(english_embeddings.vectors, W)\n",
        "\n",
        "    precision_one, precision_five = calculate_precision(lexicon, aligned_english_embeddings, hindi_embeddings)\n",
        "    ablation_results.append((size, precision_one, precision_five))\n",
        "  return ablation_results"
      ],
      "metadata": {
        "id": "fRHQTWnJmtq6"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ablation_results = ablation_study(enhi_lexicon, english_embeddings, hindi_embeddings)\n",
        "\n",
        "print(\"{Bilingual dictionary size}   {Precision@1}   {Precision@5}\")\n",
        "for size, p1, p5 in ablation_results:\n",
        "  print(f\"{size}  {p1}  {p5}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "PxEVpXX4o_ms",
        "outputId": "2adb9c00-e931-40f0-ffd4-fc1ba5da3873"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-114-5a905698f226>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mablation_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mablation_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menhi_lexicon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menglish_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhindi_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{Bilingual dictionary size}   {Precision@1}   {Precision@5}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp5\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mablation_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{size}  {p1}  {p5}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-113-78cfe60d2b6e>\u001b[0m in \u001b[0;36mablation_study\u001b[0;34m(bi_lexicon, english_embeddings, hindi_embeddings, dictionary_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0maligned_english_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mprecision_one\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_five\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_precision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlexicon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maligned_english_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhindi_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mablation_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_one\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_five\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mablation_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-103-42a0bce1aae8>\u001b[0m in \u001b[0;36mcalculate_precision\u001b[0;34m(test_data, aligned_embeddings, hindi_embeddings)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mword_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_hi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0men_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maligned_english_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menglish_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msimilar_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhindi_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# indices = similar_results[0].argsort()[-p:][::-1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilar_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     \u001b[0;31m# to avoid recursive import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1741\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, ensure_all_finite, ensure_2d, copy)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         )\n\u001b[0;32m--> 209\u001b[0;31m         Y = check_array(\n\u001b[0m\u001b[1;32m    210\u001b[0m             \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m   1108\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;31m# error message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mfirst_pass_isfinite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfirst_pass_isfinite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2311\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2313\u001b[0;31m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0m\u001b[1;32m   2314\u001b[0m                           initial=initial, where=where)\n\u001b[1;32m   2315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}